{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qkrxo\\anaconda3\\envs\\s4\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\n",
      "Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models.s4.s4 import S4Block as S4  # Can use full version instead of minimal S4D standalone below\n",
    "from models.s4.s4d import S4D\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Dropout broke in PyTorch 1.11\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) == (1, 11):\n",
    "    print(\"WARNING: Dropout is bugged in PyTorch 1.11. Results may be worse.\")\n",
    "    dropout_fn = nn.Dropout\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 12):\n",
    "    dropout_fn = nn.Dropout1d\n",
    "else:\n",
    "    dropout_fn = nn.Dropout2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import botorch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "from botorch.acquisition.analytic import LogExpectedImprovement\n",
    "from botorch.exceptions import ModelFittingError\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.generation import MaxPosteriorSampling\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.test_functions import Branin\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "dtype = torch.float\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "branin = Branin(negate=True).to(device=device, dtype=dtype)\n",
    "\n",
    "def branin_emb(x):\n",
    "    \"\"\"x is assumed to be in [-1, 1]^D\"\"\"\n",
    "    lb, ub = branin.bounds\n",
    "    return branin(lb + (ub - lb) * (x[..., :2] + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = branin_emb\n",
    "dim = 500\n",
    "\n",
    "n_init = 40\n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = (\n",
    "        2 * sobol.draw(n=n_pts).to(dtype=dtype, device=device) - 1\n",
    "    )  # points have to be in [-1, 1]^d\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class my_dataset(Dataset):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.train_x = train_x.unsqueeze(-1)\n",
    "        self.train_y = train_y\n",
    "\n",
    "        self.num = train_x.shape[0]\n",
    "\n",
    "        # Generate data\n",
    "        self.data = []\n",
    "        self.data.append((self.train_x, self.train_y))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.train_x[index], self.train_y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_candidate(\n",
    "#     state,\n",
    "#     model,  # GP model\n",
    "#     X,  # Evaluated points on the domain [-1, 1]^d\n",
    "#     Y,  # Function values\n",
    "#     n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "#     num_restarts=10,\n",
    "#     raw_samples=512,\n",
    "#     acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "# ):\n",
    "#     assert acqf in (\"ts\", \"ei\")\n",
    "#     assert X.min() >= -1.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "#     if n_candidates is None:\n",
    "#         n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "#     # Scale the TR to be proportional to the lengthscales\n",
    "#     x_center = X[Y.argmax(), :].clone()\n",
    "#     weights = model.covar_module.base_kernel.lengthscale.detach().view(-1)\n",
    "#     weights = weights / weights.mean()\n",
    "#     weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "#     tr_lb = torch.clamp(x_center - weights * state.length, -1.0, 1.0)\n",
    "#     tr_ub = torch.clamp(x_center + weights * state.length, -1.0, 1.0)\n",
    "\n",
    "#     if acqf == \"ts\":\n",
    "#         dim = X.shape[-1]\n",
    "#         sobol = SobolEngine(dim, scramble=True)\n",
    "#         pert = sobol.draw(n_candidates).to(dtype=dtype, device=device)\n",
    "#         pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "#         # Create a perturbation mask\n",
    "#         prob_perturb = min(20.0 / dim, 1.0)\n",
    "#         mask = torch.rand(n_candidates, dim, dtype=dtype, device=device) <= prob_perturb\n",
    "#         ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "#         mask[ind, torch.randint(0, dim, size=(len(ind),), device=device)] = 1\n",
    "\n",
    "#         # Create candidate points from the perturbations and the mask\n",
    "#         X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "#         X_cand[mask] = pert[mask]\n",
    "\n",
    "#         # Sample on the candidate points\n",
    "#         thompson_sampling = MaxPosteriorSampling(model=model, replacement=False)\n",
    "#         with torch.no_grad():  # We don't need gradients when using TS\n",
    "#             X_next = thompson_sampling(X_cand, num_samples=1)\n",
    "\n",
    "#     elif acqf == \"ei\":\n",
    "#         ei = LogExpectedImprovement(model, train_Y.max())\n",
    "#         X_next, acq_value = optimize_acqf(\n",
    "#             ei,\n",
    "#             bounds=torch.stack([tr_lb, tr_ub]),\n",
    "#             q=1,\n",
    "#             num_restarts=num_restarts,\n",
    "#             raw_samples=raw_samples,\n",
    "#         )\n",
    "\n",
    "#     return X_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output=10,\n",
    "        d_model=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.2,\n",
    "        prenorm=False,\n",
    "        lr=0.005\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "        self.lr = lr\n",
    "\n",
    "        # Linear encoder (d_input = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4D(d_model, dropout=dropout, transposed=True, lr=min(0.001, self.lr))\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(dropout_fn(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2) # (B, d_model, L) -> (B, L, d_model)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1) # (B, L, d_model) -> (B, d_model)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_input = 1\n",
    "d_output = 1\n",
    "d_model = 128\n",
    "n_layers = 4\n",
    "dropout = 0.1\n",
    "prenorm = True\n",
    "\n",
    "model = S4Model(\n",
    "    d_input=d_input,\n",
    "    d_output=d_output,\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    prenorm=prenorm,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "if device == 'cuda':\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 10\n",
    "dtype = torch.float\n",
    "\n",
    "train_x = get_initial_points(dim, n_init)\n",
    "train_y = torch.tensor(\n",
    "    [branin_emb(x) for x in train_x], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "val_x = get_initial_points(dim, n_init//2)\n",
    "val_y = torch.tensor(\n",
    "    [branin_emb(x) for x in val_x], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "train_data = my_dataset(train_x=train_x, train_y=train_y)\n",
    "trainloader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "\n",
    "val_data = my_dataset(train_x=val_x, train_y=val_y)\n",
    "valloader = DataLoader(val_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "pbar = tqdm(range(start_epoch, 100))\n",
    "for epoch in pbar:\n",
    "    # train()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for batch_idx, (inputs, targets) in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pbar.set_description(\n",
    "            'Training | Batch Idx: (%d/%d) | Loss: %.3f' %\n",
    "            (batch_idx, len(trainloader), train_loss/(batch_idx+1))\n",
    "        )\n",
    "\n",
    "    # val_acc = eval(epoch, valloader)\n",
    "    if epoch % 10 == 9:\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(enumerate(valloader))\n",
    "            for batch_idx, (inputs, targets) in pbar:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                pbar.set_description(\n",
    "                    'Validating | Batch Idx: (%d/%d) | Loss: %.3f' %\n",
    "                    (batch_idx, len(valloader), eval_loss/(batch_idx+1))\n",
    "                )\n",
    "\n",
    "    # scheduler.step()\n",
    "    # print(f\"Epoch {epoch} learning rate: {scheduler.get_last_lr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 256 5000\n"
     ]
    }
   ],
   "source": [
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 256 if not SMOKE_TEST else 4\n",
    "N_CANDIDATES = min(5000, max(2000, 200 * dim)) if not SMOKE_TEST else 4\n",
    "print(NUM_RESTARTS, RAW_SAMPLES, N_CANDIDATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11) Best value: -9.02e+00\n",
      "12) Best value: -4.70e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qkrxo\\anaconda3\\envs\\s4\\lib\\site-packages\\botorch\\optim\\initializers.py:432: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13) Best value: -4.70e+00\n",
      "14) Best value: -4.70e+00\n",
      "15) Best value: -4.70e+00\n",
      "16) Best value: -2.15e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qkrxo\\anaconda3\\envs\\s4\\lib\\site-packages\\botorch\\optim\\initializers.py:432: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17) Best value: -2.15e+00\n",
      "18) Best value: -2.15e+00\n",
      "19) Best value: -2.15e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qkrxo\\anaconda3\\envs\\s4\\lib\\site-packages\\botorch\\optim\\initializers.py:432: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20) Best value: -2.15e+00\n",
      "21) Best value: -2.15e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qkrxo\\anaconda3\\envs\\s4\\lib\\site-packages\\botorch\\optim\\initializers.py:432: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22) Best value: -2.15e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qkrxo\\anaconda3\\envs\\s4\\lib\\site-packages\\botorch\\optim\\initializers.py:432: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23) Best value: -2.15e+00\n",
      "24) Best value: -2.15e+00\n",
      "25) Best value: -2.15e+00\n",
      "26) Best value: -2.15e+00\n",
      "27) Best value: -2.15e+00\n",
      "28) Best value: -2.15e+00\n",
      "29) Best value: -2.15e+00\n",
      "30) Best value: -2.15e+00\n",
      "31) Best value: -2.15e+00\n",
      "32) Best value: -2.15e+00\n",
      "33) Best value: -2.15e+00\n",
      "34) Best value: -2.15e+00\n",
      "35) Best value: -1.54e+00\n",
      "36) Best value: -1.54e+00\n",
      "37) Best value: -1.54e+00\n",
      "38) Best value: -1.54e+00\n",
      "39) Best value: -1.54e+00\n",
      "40) Best value: -1.54e+00\n",
      "41) Best value: -1.54e+00\n",
      "42) Best value: -4.48e-01\n",
      "43) Best value: -4.48e-01\n",
      "44) Best value: -4.48e-01\n",
      "45) Best value: -4.48e-01\n",
      "46) Best value: -4.48e-01\n",
      "47) Best value: -4.48e-01\n",
      "48) Best value: -4.48e-01\n",
      "49) Best value: -4.48e-01\n",
      "50) Best value: -4.48e-01\n",
      "51) Best value: -4.48e-01\n",
      "52) Best value: -4.48e-01\n",
      "53) Best value: -4.48e-01\n",
      "54) Best value: -4.48e-01\n",
      "55) Best value: -4.48e-01\n",
      "56) Best value: -4.48e-01\n",
      "57) Best value: -4.48e-01\n",
      "58) Best value: -4.48e-01\n",
      "59) Best value: -4.48e-01\n",
      "60) Best value: -4.48e-01\n",
      "61) Best value: -4.48e-01\n",
      "62) Best value: -4.48e-01\n",
      "63) Best value: -4.48e-01\n",
      "64) Best value: -4.48e-01\n",
      "65) Best value: -4.48e-01\n",
      "66) Best value: -4.48e-01\n",
      "67) Best value: -4.48e-01\n",
      "68) Best value: -4.48e-01\n",
      "69) Best value: -4.48e-01\n",
      "70) Best value: -4.48e-01\n",
      "71) Best value: -4.48e-01\n",
      "72) Best value: -4.48e-01\n",
      "73) Best value: -4.48e-01\n",
      "74) Best value: -4.48e-01\n",
      "75) Best value: -4.48e-01\n",
      "76) Best value: -4.48e-01\n",
      "77) Best value: -4.48e-01\n",
      "78) Best value: -4.48e-01\n",
      "79) Best value: -4.48e-01\n",
      "80) Best value: -4.48e-01\n",
      "81) Best value: -4.48e-01\n",
      "82) Best value: -4.48e-01\n",
      "83) Best value: -4.48e-01\n",
      "84) Best value: -4.48e-01\n",
      "85) Best value: -4.48e-01\n",
      "86) Best value: -4.48e-01\n",
      "87) Best value: -4.48e-01\n",
      "88) Best value: -4.48e-01\n",
      "89) Best value: -4.48e-01\n",
      "90) Best value: -4.48e-01\n",
      "91) Best value: -4.48e-01\n",
      "92) Best value: -4.48e-01\n",
      "93) Best value: -4.48e-01\n",
      "94) Best value: -4.48e-01\n",
      "95) Best value: -4.48e-01\n",
      "96) Best value: -4.48e-01\n",
      "97) Best value: -4.48e-01\n",
      "98) Best value: -4.48e-01\n",
      "99) Best value: -4.48e-01\n",
      "100) Best value: -4.48e-01\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.double\n",
    "n_init = 10\n",
    "\n",
    "X_ei = get_initial_points(dim, n_init)\n",
    "Y_ei = torch.tensor(\n",
    "    [branin_emb(x) for x in X_ei], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "# Disable input scaling checks as we normalize to [-1, 1]\n",
    "with botorch.settings.validate_input_scaling(False):\n",
    "    while len(Y_ei) < 100:\n",
    "        train_Y = (Y_ei - Y_ei.mean()) / Y_ei.std()\n",
    "        likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "        model = SingleTaskGP(X_ei, train_Y, likelihood=likelihood)\n",
    "        mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "        optimizer = torch.optim.Adam([{\"params\": model.parameters()}], lr=0.1)\n",
    "        model.train()\n",
    "        model.likelihood.train()\n",
    "        for _ in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_ei)\n",
    "            loss = -mll(output, train_Y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Create a batch\n",
    "        ei = LogExpectedImprovement(model, train_Y.max())\n",
    "        candidate, acq_value = optimize_acqf(\n",
    "            ei,\n",
    "            bounds=torch.stack(\n",
    "                [\n",
    "                    -torch.ones(dim, dtype=dtype, device=device),\n",
    "                    torch.ones(dim, dtype=dtype, device=device),\n",
    "                ]\n",
    "            ),\n",
    "            q=1,\n",
    "            num_restarts=NUM_RESTARTS,\n",
    "            raw_samples=RAW_SAMPLES,\n",
    "        )\n",
    "        Y_next = torch.tensor(\n",
    "            [branin_emb(x) for x in candidate], dtype=dtype, device=device\n",
    "        ).unsqueeze(-1)\n",
    "\n",
    "        # Append data\n",
    "        X_ei = torch.cat((X_ei, candidate), axis=0)\n",
    "        Y_ei = torch.cat((Y_ei, Y_next), axis=0)\n",
    "\n",
    "        # Print current status\n",
    "        print(f\"{len(X_ei)}) Best value: {Y_ei.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Sobol = (\n",
    "    SobolEngine(dim, scramble=True, seed=0)\n",
    "    .draw(len(X_ei))\n",
    "    .to(dtype=dtype, device=device)\n",
    "    * 2\n",
    "    - 1\n",
    ")\n",
    "Y_Sobol = torch.tensor(\n",
    "    [branin_emb(x) for x in X_Sobol], dtype=dtype, device=device\n",
    ").unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "names = [\"GP-qEI\", \"Sobol\"] # , \"EI\", \"Sobol\"\n",
    "runs = [Y_ei, Y_Sobol] # , Y_ei, Y_Sobol\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, run in zip(names, runs):\n",
    "    fx = np.maximum.accumulate(run.cpu())\n",
    "    plt.plot(fx, marker=\"\", lw=3)\n",
    "\n",
    "plt.plot([0, len(Y_ei)], [fun.optimal_value, fun.optimal_value], \"k--\", lw=3)\n",
    "plt.xlabel(\"Function value\", fontsize=18)\n",
    "plt.xlabel(\"Number of evaluations\", fontsize=18)\n",
    "plt.title(\"20D Ackley\", fontsize=24)\n",
    "plt.xlim([0, len(Y_ei)])\n",
    "plt.ylim([-15, 1])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend(\n",
    "    names + [\"Global optimal value\"],\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0, -0.08, 1, 1),\n",
    "    bbox_transform=plt.gcf().transFigure,\n",
    "    ncol=4,\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# names = [\"EI\"] # , \"Sobol\"\n",
    "# runs = [Y_ei] #, Y_Sobol\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# for name, run in zip(names, runs):\n",
    "#     fx = np.maximum.accumulate(run.cpu())\n",
    "#     plt.plot(-fx + branin.optimal_value, marker=\"\", lw=3)\n",
    "\n",
    "# plt.ylabel(\"Regret\", fontsize=18)\n",
    "# plt.xlabel(\"Number of evaluations\", fontsize=18)\n",
    "# plt.title(f\"{dim}D Embedded Branin\", fontsize=24)\n",
    "# plt.xlim([0, len(Y_ei)])\n",
    "# plt.yscale(\"log\")\n",
    "\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.legend(\n",
    "#     names + [\"Global optimal value\"],\n",
    "#     loc=\"lower center\",\n",
    "#     bbox_to_anchor=(0, -0.08, 1, 1),\n",
    "#     bbox_transform=plt.gcf().transFigure,\n",
    "#     ncol=4,\n",
    "#     fontsize=16,\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
